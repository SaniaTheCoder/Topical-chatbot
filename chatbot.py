# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-nM6Qi4az6UX4dQrWAPbgokFJz7PPee8
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import json
import os

import tensorflow as tf

# Get the list of available GPUs
gpus = tf.config.experimental.list_physical_devices('GPU')

if gpus:
    try:
        # Set memory growth to False for each GPU
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, False)

        # Log how much memory is allocated
        print("All available GPU memory will be allocated.")
    except RuntimeError as e:
        # Memory growth must be set before initializing GPUs
        print(e)

json_files = ["/content/drive/MyDrive/DSAI_Gilgit/NLP Project/NLP_project/valid_freq.json", "/content/drive/MyDrive/DSAI_Gilgit/NLP Project/NLP_project/valid_rare.json"]
data=[]

def process_json(file):
    with open(file, 'r') as f:
        conversations = json.load(f)  # Load the JSON data

        # Iterate over each conversation in the file
        for conv_id, conv_data in conversations.items():
            content = conv_data['content']  # Access the conversation turns
            for turn in content:
                # Extract relevant fields from each conversation turn
                message = turn.get('message', '')
                agent = turn.get('agent', '')
                sentiment = turn.get('sentiment', '')
                turn_rating = turn.get('turn_rating', '')
                knowledge_source = turn.get('knowledge_source', [])

                # Append the structured data as a row
                data.append({
                    'message': message,
                    'agent': agent,
                    'sentiment': sentiment,
                    'turn_rating': turn_rating,
                    'knowledge_source': ', '.join(knowledge_source)  # Join list as a string
                })

for json_file in json_files:
    process_json(json_file)

df = pd.DataFrame(data)
df.to_csv('structured_conversations.csv', index=False)
print(df.head)

from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.normalizers import NFKC
from tokenizers.processors import BertProcessing
from tokenizers.trainers import BpeTrainer
import pandas as pd

df = pd.read_csv('structured_conversations.csv')
texts = df['message'].tolist()
tokenizer = Tokenizer(models.BPE())
tokenizer.normalizer = NFKC()
tokenizer.pre_tokenizer = Whitespace()
trainer = BpeTrainer(vocab_size=10000, special_tokens=["<s>", "<pad>", "</s>", "<unk>"])
tokenizer.train_from_iterator(texts, trainer=trainer)
tokenizer.post_processor = BertProcessing(("</s>", tokenizer.token_to_id("</s>")),
                                          ("<s>", tokenizer.token_to_id("<s>")))
tokenizer.save("bpe_tokenizer.json")
encoded = tokenizer.encode("Both are excellent technology they are helpful in many ways.")
print("Tokenized IDs:", encoded.ids)
print("Tokenized Tokens:", encoded.tokens)
df['tokenized_message'] = df['message'].apply(lambda x: tokenizer.encode(x).tokens)
df.to_csv('tokenized_conversations.csv', index=False)

from tokenizers import Tokenizer, models, trainers, pre_tokenizers
import pandas as pd

df = pd.read_csv('structured_conversations.csv')
texts = df['message'].tolist()
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
trainer = trainers.BpeTrainer(vocab_size=20000, special_tokens=["<s>", "<pad>", "</s>", "<unk>"])
tokenizer.train_from_iterator(texts, trainer=trainer)
tokenizer.save("bpe_tokenizer.json")

import tensorflow as tf
from tokenizers import Tokenizer
import numpy as np
import pandas as pd

tokenizer = Tokenizer.from_file("bpe_tokenizer.json")
df = pd.read_csv('structured_conversations.csv')
def tokenize_texts(texts, tokenizer, max_length=130):
    tokenized = []
    for text in texts:
        tokens = tokenizer.encode(text).ids
        # Truncate (post-truncation) if longer than max_length
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
        tokenized.append(tokens)
    return tokenized
texts = df['message'].tolist()
tokenized_sequences = tokenize_texts(texts, tokenizer, max_length=130)
padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(
    tokenized_sequences, maxlen=130, padding='pre', truncating='post', value=tokenizer.token_to_id("<pad>")
)
attention_masks = np.where(padded_sequences != tokenizer.token_to_id("<pad>"), 1, 0)
input_sequences = tf.convert_to_tensor(padded_sequences)
attention_masks = tf.convert_to_tensor(attention_masks)
batch_size = 32  # Set your batch size
dataset = tf.data.Dataset.from_tensor_slices((input_sequences, attention_masks))
dataset = dataset.shuffle(buffer_size=len(input_sequences)).batch(batch_size)
for batch_input, batch_mask in dataset.take(1):
    print("Input batch shape:", batch_input.shape)
    print("Attention mask batch shape:", batch_mask.shape)

import numpy as np
import tensorflow as tf

glove_file_path = '/content/drive/MyDrive/DSAI_Gilgit/NLP Project/NLP_project/glove.6B.200d.txt'
embedding_dim = 200
vocab_size = 20000  # Ensure this matches the size of your tokenizer's vocabulary
glove_embeddings_index = {}
with open(glove_file_path, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], dtype='float32')
        glove_embeddings_index[word] = vector

print(f"Loaded {len(glove_embeddings_index)} word vectors from GloVe.")

embedding_matrix = np.zeros((vocab_size, embedding_dim))
tokenizer = tf.keras.preprocessing.text.Tokenizer()
for word, index in tokenizer.word_index.items():
    if index < vocab_size:
        embedding_vector = glove_embeddings_index.get(word)
        if embedding_vector is not None:
            # Words found in GloVe will have their embeddings loaded
            embedding_matrix[index] = embedding_vector
        else:
            # Words not found in GloVe will be left as zeros or can be randomly initialized
            embedding_matrix[index] = np.random.normal(size=(embedding_dim,))

print(f"Created an embedding matrix of shape {embedding_matrix.shape}.")

print(embedding_matrix)

embedding_layer = tf.keras.layers.Embedding(
    input_dim=vocab_size,
    output_dim=embedding_dim,
    weights=[embedding_matrix],
    input_length=130,  # Maximum sequence length
    trainable=False  # Set to False if you don't want to update the GloVe embeddings during training
)

!pip install datasets

import tensorflow as tf
from transformers import GPT2Tokenizer, TFGPT2LMHeadModel
from datasets import Dataset
import pandas as pd

# Load GPT-2 tokenizer from Hugging Face
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token  # Set PAD token to EOS token

# Load GPT-2 model
model = TFGPT2LMHeadModel.from_pretrained('gpt2')

# Maximum sequence length and batch size
MAX_SEQ_LENGTH = 130
BATCH_SIZE = 32

# Function to tokenize and encode input text
def tokenize_function(examples):
    return tokenizer(
        examples["message"],
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQ_LENGTH,
        return_tensors='tf',  # Ensure to return tensors for TensorFlow
    )

# Load your dataset (assuming it's saved as a CSV file)
df = pd.read_csv('/content/drive/MyDrive/DSAI_Gilgit/NLP Project/NLP_project/structured_conversations.csv')

# Convert the DataFrame to a Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Tokenize the dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['agent', 'sentiment', 'turn_rating', 'knowledge_source'])

# Convert tokenized dataset to TensorFlow format
def generate_tf_dataset(tokenized_data, batch_size, shuffle=True):
    input_ids = tokenized_data['input_ids']
    attention_masks = tokenized_data['attention_mask']

    dataset = tf.data.Dataset.from_tensor_slices(({
        'input_ids': input_ids, 'attention_mask': attention_masks
    }, input_ids))  # Model needs inputs and labels (here labels are same as input_ids)

    if shuffle:
        dataset = dataset.shuffle(buffer_size=len(input_ids))

    dataset = dataset.batch(batch_size)
    return dataset

# Create TensorFlow dataset for training
tf_dataset = generate_tf_dataset(tokenized_dataset, BATCH_SIZE)

import tensorflow as tf
print(tf.__version__)

!pip install --upgrade tensorflow

import tensorflow as tf

# Ensure TensorFlow version
print("TensorFlow version:", tf.__version__)

# Example model for testing
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
    tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10)
])

# Use standard Adam optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)

# Compile with built-in loss function to ensure compatibility
model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

# Test dataset (replace `tf_dataset` with a valid dataset)
# Example dummy dataset:
import numpy as np
x_train = np.random.random((32, 28, 28, 1))  # Replace with your dataset
y_train = np.random.randint(0, 10, (32,))
tf_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(8)

# Fit the model
history = model.fit(tf_dataset, epochs=20, verbose=1)

# Response Accuracy

import numpy as np

# Simulated predictions and true labels
true_responses = ["How can I help you?", "Sure, let me look into that.", "Thank you!"]
predicted_responses = ["How can I help you?", "Sure, let me look into that.", "Thanks!"]

# Calculate accuracy
correct = sum([1 for true, pred in zip(true_responses, predicted_responses) if true == pred])
accuracy = correct / len(true_responses)
print(f"Response Accuracy: {accuracy:.2f}")

# Precision, Recall, and F1 Score

from sklearn.metrics import precision_score, recall_score, f1_score

# Binary classification: 1 = relevant, 0 = irrelevant
true_labels = [1, 0, 1, 1, 0]
predicted_labels = [1, 0, 1, 0, 0]

precision = precision_score(true_labels, predicted_labels)
recall = recall_score(true_labels, predicted_labels)
f1 = f1_score(true_labels, predicted_labels)

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Sentiment Analysis

from transformers import pipeline

# Load sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

# Example user responses
user_responses = ["I love this chatbot!", "This is frustrating.", "It's okay, I guess."]
sentiments = sentiment_pipeline(user_responses)

print("Sentiment Analysis Results:")
for response, sentiment in zip(user_responses, sentiments):
    print(f"Response: {response}\nSentiment: {sentiment}\n")

# Engagement Metrics

session_interactions = [5, 10, 7]  # Number of interactions in 3 sessions
session_durations = [120, 200, 150]  # Duration in seconds

avg_turn_length = sum(session_durations) / sum(session_interactions)
avg_session_duration = np.mean(session_durations)

print(f"Average Turn Length: {avg_turn_length:.2f} seconds")
print(f"Average Session Duration: {avg_session_duration:.2f} seconds")

# Fallback Rate

total_queries = 100
fallbacks = 10

fallback_rate = fallbacks / total_queries
print(f"Fallback Rate: {fallback_rate:.2%}")

# Completion Rate

total_tasks = 50
completed_tasks = 45

completion_rate = completed_tasks / total_tasks
print(f"Completion Rate: {completion_rate:.2%}")

# Churn Rate

total_users = 100
users_who_churned = 20

churn_rate = users_who_churned / total_users
print(f"Churn Rate: {churn_rate:.2%}")

# Human Handoff Rate

total_sessions = 50
human_handoffs = 5

handoff_rate = human_handoffs / total_sessions
print(f"Human Handoff Rate: {handoff_rate:.2%}")

# Task Completion Rate

tasks_completed = 30
tasks_attempted = 35

task_completion_rate = tasks_completed / tasks_attempted
print(f"Task Completion Rate: {task_completion_rate:.2%}")

# Subjective Evaluation

user_ratings = [4, 5, 3, 4, 5]
avg_user_satisfaction = np.mean(user_ratings)
print(f"Average User Satisfaction: {avg_user_satisfaction:.2f}")

